{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protected-accommodation",
   "metadata": {},
   "source": [
    "### Matthew Collins\n",
    "#### DSC650: Week 3\n",
    "#### Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "educational-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import json\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "import pandas as pd\n",
    "import s3fs\n",
    "import pyarrow as pa\n",
    "from pyarrow.json import read_json\n",
    "import pyarrow.parquet as pq\n",
    "import fastavro\n",
    "import pygeohash\n",
    "import snappy\n",
    "import jsonschema\n",
    "from jsonschema.exceptions import ValidationError\n",
    "\n",
    "# Create directories\n",
    "\n",
    "endpoint_url='https://storage.budsc.midwest-datascience.com'\n",
    "\n",
    "current_dir = Path(os.getcwd()).absolute()\n",
    "schema_dir = current_dir.joinpath('schemas')\n",
    "results_dir = current_dir.joinpath('results')\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create function to get and load the data\n",
    "\n",
    "def read_jsonl_data():\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            records = [json.loads(line) for line in f.readlines()]\n",
    "        \n",
    "\n",
    "    return records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "painted-pasta",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"airline\": {\n",
      "        \"airline_id\": 410,\n",
      "        \"name\": \"Aerocondor\",\n",
      "        \"alias\": \"ANA All Nippon Airways\",\n",
      "        \"iata\": \"2B\",\n",
      "        \"icao\": \"ARD\",\n",
      "        \"callsign\": \"AEROCONDOR\",\n",
      "        \"country\": \"Portugal\",\n",
      "        \"active\": true\n",
      "    },\n",
      "    \"src_airport\": {\n",
      "        \"airport_id\": 2966,\n",
      "        \"name\": \"Astrakhan Airport\",\n",
      "        \"city\": \"Astrakhan\",\n",
      "        \"country\": \"Russia\",\n",
      "        \"iata\": \"ASF\",\n",
      "        \"icao\": \"URWA\",\n",
      "        \"latitude\": 46.2832984924,\n",
      "        \"longitude\": 48.0063018799,\n",
      "        \"altitude\": -65,\n",
      "        \"timezone\": 4.0,\n",
      "        \"dst\": \"N\",\n",
      "        \"tz_id\": \"Europe/Samara\",\n",
      "        \"type\": \"airport\",\n",
      "        \"source\": \"OurAirports\"\n",
      "    },\n",
      "    \"dst_airport\": {\n",
      "        \"airport_id\": 2990,\n",
      "        \"name\": \"Kazan International Airport\",\n",
      "        \"city\": \"Kazan\",\n",
      "        \"country\": \"Russia\",\n",
      "        \"iata\": \"KZN\",\n",
      "        \"icao\": \"UWKD\",\n",
      "        \"latitude\": 55.606201171875,\n",
      "        \"longitude\": 49.278701782227,\n",
      "        \"altitude\": 411,\n",
      "        \"timezone\": 3.0,\n",
      "        \"dst\": \"N\",\n",
      "        \"tz_id\": \"Europe/Moscow\",\n",
      "        \"type\": \"airport\",\n",
      "        \"source\": \"OurAirports\"\n",
      "    },\n",
      "    \"codeshare\": false,\n",
      "    \"equipment\": [\n",
      "        \"CR2\"\n",
      "    ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load the file and view it.  I used this information to create the initial json schema\n",
    "\n",
    "records = read_jsonl_data()\n",
    "print(json.dumps(records[1], sort_keys=False, indent = 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marked-diameter",
   "metadata": {},
   "source": [
    "### 3.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ideal-example",
   "metadata": {},
   "source": [
    "#### 3.1a JSON Scehma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "illegal-aquarium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to develop the JSON schema\n",
    "\n",
    "def validate_jsonl_data(records):\n",
    "    schema_path = schema_dir.joinpath('routes-schema.json')\n",
    "    with open(schema_path) as f:\n",
    "        schema = json.load(f)\n",
    "          \n",
    "    # create variable to write csv file\n",
    "    validation_csv_path = results_dir.joinpath('validation-results.csv') \n",
    "    with open(validation_csv_path, 'w') as f:    \n",
    "        fieldnames = ['row_num', 'is_valid', 'msg'] #'airline', 'src_airport', 'dst_airport', 'codeshare', 'equipment'] # create column names           \n",
    "        csv_writer = csv.writer(f)             # write to file \n",
    "        csv_writer.writerow(fieldnames)        # add column names to the file\n",
    "        \n",
    "        \n",
    "        for i, record in enumerate(records):\n",
    "\n",
    "            try:                \n",
    "                ## TODO validate the csv path\n",
    "                jsonschema.validate(instance = record, schema = schema)\n",
    "                result = dict(\n",
    "                    row_num = i,\n",
    "                    is_valid = True,\n",
    "                    msg = record\n",
    "                )\n",
    "\n",
    "                \n",
    "                ## Added code above\n",
    "                #pass\n",
    "            \n",
    "            except ValidationError as e:\n",
    "                ## Print message if invalid record\n",
    "                #print('JSON is invalid')\n",
    "                #pass\n",
    "                result = dict(\n",
    "                    row_num = i,\n",
    "                    is_valid = False,\n",
    "                    msg = e\n",
    "                )\n",
    "     \n",
    "            finally:                \n",
    "                csv_writer.writerow(result.values())\n",
    "    \n",
    "validate_jsonl_data(records)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "specified-transport",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    67663\n",
       "Name: is_valid, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Provide the outcome information for the validation-results.csv counts to ensure schema fits data\n",
    "\n",
    "pd.read_csv('results/validation-results.csv')['is_valid'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "exact-beauty",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results/validation-results.csv')\n",
    "df = df.loc[df['is_valid'] == False]\n",
    "df.to_csv('results/df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "filled-filing",
   "metadata": {},
   "source": [
    "#### 3.1b Avro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "adaptive-tribute",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to develop Avro schema\n",
    "\n",
    "#import additional elements from fastavro\n",
    "from fastavro import writer, reader, parse_schema\n",
    "\n",
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routes.avsc')\n",
    "    data_path = results_dir.joinpath('routes.avro')\n",
    "    ## TODO: Use fastavro to create Avro dataset\n",
    "    \n",
    "    with open(schema_path, 'r') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    parsed_schema = parse_schema(schema)\n",
    "    \n",
    "    with open(data_path, 'wb') as out:\n",
    "        writer(out, parsed_schema, records)\n",
    "    \n",
    "    #with open(data_path, 'rb') as fo:\n",
    "        #for record in reader(fo):\n",
    "            #print(record)      \n",
    "    \n",
    "    ## Added code above    \n",
    "      \n",
    "create_avro_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grateful-hormone",
   "metadata": {},
   "source": [
    "#### 3.1c Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "military-causing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function develop Parquet schema\n",
    "\n",
    "def create_parquet_dataset():\n",
    "    src_data_path = 'data/processed/openflights/routes.jsonl.gz'\n",
    "    parquet_output_path = results_dir.joinpath('routes.parquet')\n",
    "    s3 = s3fs.S3FileSystem(\n",
    "        anon=True,\n",
    "        client_kwargs={\n",
    "            'endpoint_url': endpoint_url\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    with s3.open(src_data_path, 'rb') as f_gz:\n",
    "        with gzip.open(f_gz, 'rb') as f:\n",
    "            #records = [json.loads(line) for line in f.readlines()]\n",
    "                        \n",
    "            #pass\n",
    "            ## TODO: Use Apache Arrow to create Parquet table and save the dataset\n",
    "            file = read_json(f)\n",
    "            pq.write_table(file, parquet_output_path)\n",
    "            \n",
    "            ## Added code above\n",
    "\n",
    "create_parquet_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "skilled-bosnia",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline</th>\n",
       "      <th>src_airport</th>\n",
       "      <th>dst_airport</th>\n",
       "      <th>codeshare</th>\n",
       "      <th>equipment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'airline_id': 410, 'name': 'Aerocondor', 'ali...</td>\n",
       "      <td>{'airport_id': 2965.0, 'name': 'Sochi Internat...</td>\n",
       "      <td>{'airport_id': 2990.0, 'name': 'Kazan Internat...</td>\n",
       "      <td>False</td>\n",
       "      <td>[CR2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'airline_id': 410, 'name': 'Aerocondor', 'ali...</td>\n",
       "      <td>{'airport_id': 2966.0, 'name': 'Astrakhan Airp...</td>\n",
       "      <td>{'airport_id': 2990.0, 'name': 'Kazan Internat...</td>\n",
       "      <td>False</td>\n",
       "      <td>[CR2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'airline_id': 410, 'name': 'Aerocondor', 'ali...</td>\n",
       "      <td>{'airport_id': 2966.0, 'name': 'Astrakhan Airp...</td>\n",
       "      <td>{'airport_id': 2962.0, 'name': 'Mineralnyye Vo...</td>\n",
       "      <td>False</td>\n",
       "      <td>[CR2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'airline_id': 410, 'name': 'Aerocondor', 'ali...</td>\n",
       "      <td>{'airport_id': 2968.0, 'name': 'Chelyabinsk Ba...</td>\n",
       "      <td>{'airport_id': 2990.0, 'name': 'Kazan Internat...</td>\n",
       "      <td>False</td>\n",
       "      <td>[CR2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'airline_id': 410, 'name': 'Aerocondor', 'ali...</td>\n",
       "      <td>{'airport_id': 2968.0, 'name': 'Chelyabinsk Ba...</td>\n",
       "      <td>{'airport_id': 4078.0, 'name': 'Tolmachevo Air...</td>\n",
       "      <td>False</td>\n",
       "      <td>[CR2]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67658</th>\n",
       "      <td>{'airline_id': 4178, 'name': 'Regional Express...</td>\n",
       "      <td>{'airport_id': 6334.0, 'name': 'Whyalla Airpor...</td>\n",
       "      <td>{'airport_id': 3341.0, 'name': 'Adelaide Inter...</td>\n",
       "      <td>False</td>\n",
       "      <td>[SF3]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67659</th>\n",
       "      <td>{'airline_id': 19016, 'name': 'Apache Air', 'a...</td>\n",
       "      <td>{'airport_id': 4029.0, 'name': 'Domodedovo Int...</td>\n",
       "      <td>{'airport_id': 2912.0, 'name': 'Manas Internat...</td>\n",
       "      <td>False</td>\n",
       "      <td>[734]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67660</th>\n",
       "      <td>{'airline_id': 19016, 'name': 'Apache Air', 'a...</td>\n",
       "      <td>{'airport_id': 2912.0, 'name': 'Manas Internat...</td>\n",
       "      <td>{'airport_id': 4029.0, 'name': 'Domodedovo Int...</td>\n",
       "      <td>False</td>\n",
       "      <td>[734]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67661</th>\n",
       "      <td>{'airline_id': 19016, 'name': 'Apache Air', 'a...</td>\n",
       "      <td>{'airport_id': 2912.0, 'name': 'Manas Internat...</td>\n",
       "      <td>{'airport_id': 2913.0, 'name': 'Osh Airport', ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[734]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67662</th>\n",
       "      <td>{'airline_id': 19016, 'name': 'Apache Air', 'a...</td>\n",
       "      <td>{'airport_id': 2913.0, 'name': 'Osh Airport', ...</td>\n",
       "      <td>{'airport_id': 2912.0, 'name': 'Manas Internat...</td>\n",
       "      <td>False</td>\n",
       "      <td>[734]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67663 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 airline  \\\n",
       "0      {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
       "1      {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
       "2      {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
       "3      {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
       "4      {'airline_id': 410, 'name': 'Aerocondor', 'ali...   \n",
       "...                                                  ...   \n",
       "67658  {'airline_id': 4178, 'name': 'Regional Express...   \n",
       "67659  {'airline_id': 19016, 'name': 'Apache Air', 'a...   \n",
       "67660  {'airline_id': 19016, 'name': 'Apache Air', 'a...   \n",
       "67661  {'airline_id': 19016, 'name': 'Apache Air', 'a...   \n",
       "67662  {'airline_id': 19016, 'name': 'Apache Air', 'a...   \n",
       "\n",
       "                                             src_airport  \\\n",
       "0      {'airport_id': 2965.0, 'name': 'Sochi Internat...   \n",
       "1      {'airport_id': 2966.0, 'name': 'Astrakhan Airp...   \n",
       "2      {'airport_id': 2966.0, 'name': 'Astrakhan Airp...   \n",
       "3      {'airport_id': 2968.0, 'name': 'Chelyabinsk Ba...   \n",
       "4      {'airport_id': 2968.0, 'name': 'Chelyabinsk Ba...   \n",
       "...                                                  ...   \n",
       "67658  {'airport_id': 6334.0, 'name': 'Whyalla Airpor...   \n",
       "67659  {'airport_id': 4029.0, 'name': 'Domodedovo Int...   \n",
       "67660  {'airport_id': 2912.0, 'name': 'Manas Internat...   \n",
       "67661  {'airport_id': 2912.0, 'name': 'Manas Internat...   \n",
       "67662  {'airport_id': 2913.0, 'name': 'Osh Airport', ...   \n",
       "\n",
       "                                             dst_airport  codeshare equipment  \n",
       "0      {'airport_id': 2990.0, 'name': 'Kazan Internat...      False     [CR2]  \n",
       "1      {'airport_id': 2990.0, 'name': 'Kazan Internat...      False     [CR2]  \n",
       "2      {'airport_id': 2962.0, 'name': 'Mineralnyye Vo...      False     [CR2]  \n",
       "3      {'airport_id': 2990.0, 'name': 'Kazan Internat...      False     [CR2]  \n",
       "4      {'airport_id': 4078.0, 'name': 'Tolmachevo Air...      False     [CR2]  \n",
       "...                                                  ...        ...       ...  \n",
       "67658  {'airport_id': 3341.0, 'name': 'Adelaide Inter...      False     [SF3]  \n",
       "67659  {'airport_id': 2912.0, 'name': 'Manas Internat...      False     [734]  \n",
       "67660  {'airport_id': 4029.0, 'name': 'Domodedovo Int...      False     [734]  \n",
       "67661  {'airport_id': 2913.0, 'name': 'Osh Airport', ...      False     [734]  \n",
       "67662  {'airport_id': 2912.0, 'name': 'Manas Internat...      False     [734]  \n",
       "\n",
       "[67663 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the Parquet dataset to ensure it exist (I cannot read binary)\n",
    "\n",
    "pq_output = results_dir.joinpath('routes.parquet')\n",
    "table2 = pq.read_table(pq_output)\n",
    "table2.to_pandas()\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "another-smell",
   "metadata": {},
   "source": [
    "#### 3.1d Protocol Buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "continent-norwegian",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, os.path.abspath('routes_pb2'))\n",
    "\n",
    "import routes_pb2\n",
    "\n",
    "def _airport_to_proto_obj(airport):\n",
    "    obj = routes_pb2.Airport()\n",
    "    if airport is None:\n",
    "        return None\n",
    "    if airport.get('airport_id') is None:        \n",
    "        return None\n",
    "\n",
    "    obj.airport_id = airport.get('airport_id')\n",
    "    if airport.get('name'):\n",
    "        obj.name = airport.get('name')\n",
    "    if airport.get('city'):\n",
    "        obj.city = airport.get('city')\n",
    "    if airport.get('iata'):\n",
    "        obj.iata = airport.get('iata')\n",
    "    if airport.get('icao'):\n",
    "        obj.icao = airport.get('icao')\n",
    "    if airport.get('altitude'):\n",
    "        obj.altitude = airport.get('altitude')\n",
    "    if airport.get('timezone'):\n",
    "        obj.timezone = airport.get('timezone')\n",
    "    if airport.get('dst'):\n",
    "        obj.dst = airport.get('dst')\n",
    "    if airport.get('tz_id'):\n",
    "        obj.tz_id = airport.get('tz_id')\n",
    "    if airport.get('type'):\n",
    "        obj.type = airport.get('type')\n",
    "    if airport.get('source'):\n",
    "        obj.source = airport.get('source')\n",
    "\n",
    "    obj.latitude = airport.get('latitude')\n",
    "    obj.longitude = airport.get('longitude')\n",
    "\n",
    "    return obj\n",
    "\n",
    "\n",
    "def _airline_to_proto_obj(airline):\n",
    "    obj = routes_pb2.Airline()\n",
    "    ## TODO: Create an Airline obj using Protocol Buffers API\n",
    "    #if airline is None:\n",
    "        #return obj\n",
    "    obj.airline_id = airline.get('airline_id')  # required\n",
    "    obj.name = airline.get('name')   # required    \n",
    "    \n",
    "    #alias optional\n",
    "    if airline.get('alias'):\n",
    "        obj.alias = airline.get('alias')        \n",
    "    if not airline.get('alias'):\n",
    "        return None\n",
    "    \n",
    "    # iata optional\n",
    "    if airline.get('iata'):\n",
    "        obj.iata = airline.get('iata')        \n",
    "    if not airline.get('iata'):\n",
    "        return None\n",
    "    \n",
    "    # icao optional\n",
    "    if airline.get('icao'):\n",
    "        obj.iata = airline.get('iata')        \n",
    "    if not airline.get('icao'):\n",
    "        return None\n",
    "    \n",
    "    # callsign optional\n",
    "    if airline.get('callsign'):\n",
    "        obj.iata = airline.get('callsign')        \n",
    "    if not airline.get('callsign'):\n",
    "        return None\n",
    "\n",
    "    # country optional\n",
    "    if airline.get('country'):\n",
    "        obj.iata = airline.get('country')        \n",
    "    if not airline.get('country'):\n",
    "        return None    \n",
    "    \n",
    "    # active required\n",
    "    obj.active = airline.get('active') # required    \n",
    "    ## Added code above\n",
    "    return obj\n",
    "\n",
    "\n",
    "\n",
    "def create_protobuf_dataset(records):\n",
    "    routes = routes_pb2.Routes()\n",
    "    for record in records:\n",
    "        route = routes_pb2.Route()\n",
    "        \n",
    "        ## Create code to grab the Protocol Buffers Dataset        \n",
    "        airline = _airline_to_proto_obj(record.get('airline', {}))        \n",
    "        if airline:\n",
    "            route.airline.CopyFrom(airline)\n",
    "            \n",
    "        src_airport = _airport_to_proto_obj(record.get('src_airport', {}))        \n",
    "        if src_airport:\n",
    "            route.src_airport.CopyFrom(src_airport)\n",
    "        \n",
    "        dst_airport = _airport_to_proto_obj(record.get('dst_airport', {}))        \n",
    "        if dst_airport:\n",
    "            route.dst_airport.CopyFrom(dst_airport)\n",
    "        \n",
    "        codeshare = record.get('codeshare')\n",
    "        route.codeshare = record['codeshare']\n",
    "            \n",
    "        equipment = record.get('equipment')\n",
    "        route.equipment.extend(equipment)  # repeated value can only be appended or extended\n",
    "        \n",
    "        routes.route.append(route)\n",
    "\n",
    "    data_path = results_dir.joinpath('routes.pb')\n",
    "\n",
    "    with open(data_path, 'wb') as f:\n",
    "        f.write(routes.SerializeToString())\n",
    "        \n",
    "    compressed_path = results_dir.joinpath('routes.pb.snappy')\n",
    "    \n",
    "    with open(compressed_path, 'wb') as f:\n",
    "        f.write(snappy.compress(routes.SerializeToString()))\n",
    "        \n",
    "create_protobuf_dataset(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-newton",
   "metadata": {},
   "source": [
    "#### 3.1e Output sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "loaded-stream",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   df.csv: 4.100471MB\n",
      "           routes.parquet: 1.883954MB\n",
      "              routes.avro: 18.736102MB\n",
      "   validation-results.csv: 60.772452MB\n"
     ]
    }
   ],
   "source": [
    "# Get size of files\n",
    "\n",
    "comparison_csv_path = results_dir.joinpath('comparison.csv')\n",
    "\n",
    "# Create function to compare file sizes\n",
    "with open(comparison_csv_path, 'w', encoding = 'utf-8') as f:\n",
    "    fieldnames = ['file', 'size_MB'] # column names\n",
    "    \n",
    "    csv_writer = csv.DictWriter(f, fieldnames = fieldnames, lineterminator = '\\n') # writer object\n",
    "    csv_writer.writeheader()  # write column heads\n",
    "    \n",
    "    for files in os.walk(results_dir):\n",
    "        try:\n",
    "            for file in files[2]:\n",
    "                if file == 'comparison.csv':\n",
    "                    next\n",
    "                    \n",
    "                else:\n",
    "                    print(f'{file:>25}: {int(os.stat(results_dir.joinpath(file))[6]) / 1024 / 1024:2f}MB')\n",
    "                    \n",
    "                    file_size = dict(file = file, size_MB = int(os.stat(results_dir.joinpath(file))[6]) / 1024 / 1024)  # make a dict to write to csv file\n",
    "                    csv_writer.writerow(file_size) # write to csv file\n",
    "                    \n",
    "        except FileNotFoundError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinate-patrol",
   "metadata": {},
   "source": [
    "### 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-screening",
   "metadata": {},
   "source": [
    "#### 3.2a Create a Simple Geohash Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "played-tiger",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_hash_dirs(records):\n",
    "    geoindex_dir = results_dir.joinpath('geoindex')\n",
    "    geoindex_dir.mkdir(exist_ok=True, parents=True)\n",
    "    hashes = []\n",
    "    ## TODO: Create hash index\n",
    "\n",
    "    for record in records:\n",
    "        src_airport = record.get('src_airport', {})\n",
    "        if src_airport:\n",
    "            latitude = src_airport.get('latitude')\n",
    "            longitude = src_airport.get('longitude')\n",
    "            if latitude and longitude:                               \n",
    "                geohash = pygeohash.encode(latitude, longitude, precision = 5)\n",
    "                hashes.append(geohash)\n",
    "                record['geohash'] = geohash                \n",
    "                ## Added code above\n",
    "                      \n",
    "    hashes.sort()\n",
    "    three_letter = sorted(list(set([entry[:3] for entry in hashes])))\n",
    "    hash_index = {value: [] for value in three_letter}\n",
    "    for record in records:\n",
    "        geohash = record.get('geohash')\n",
    "        if geohash:\n",
    "            hash_index[geohash[:3]].append(record)\n",
    "        \n",
    "    for key, values in hash_index.items():\n",
    "        output_dir = geoindex_dir.joinpath(str(key[:1])).joinpath(str(key[:2]))\n",
    "        output_dir.mkdir(exist_ok=True, parents=True)\n",
    "        output_path = output_dir.joinpath('{}.jsonl.gz'.format(key))\n",
    "        with gzip.open(output_path, 'w') as f:\n",
    "            json_output = '\\n'.join([json.dumps(value) for value in values])\n",
    "            f.write(json_output.encode('utf-8'))\n",
    "    \n",
    "\n",
    "create_hash_dirs(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-retro",
   "metadata": {},
   "source": [
    "#### 3.2b Implement a simple search feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "convertible-portsmouth",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'airport_id': 3454,\n",
       "  'name': 'Eppley Airfield',\n",
       "  'city': 'Omaha',\n",
       "  'country': 'United States',\n",
       "  'iata': 'OMA',\n",
       "  'icao': 'KOMA',\n",
       "  'latitude': 41.3032,\n",
       "  'longitude': -95.89409599999999,\n",
       "  'altitude': 984,\n",
       "  'timezone': -6.0,\n",
       "  'dst': 'A',\n",
       "  'tz_id': 'America/Chicago',\n",
       "  'type': 'airport',\n",
       "  'source': 'OurAirports'},\n",
       " ['9z7fc'])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def airport_search(latitude, longitude):\n",
    "    '''\n",
    "    create a search function using geohash values\n",
    "    latitutd and longitude     \n",
    "    '''    \n",
    "    # Create simple search to return nearest airports under 50 KMs\n",
    "    # Convert lat and Long into geohash        \n",
    "    search_hash = pygeohash.encode(latitude, longitude)\n",
    "    \n",
    "    # Use geohash to find airports within 50 Kilometers           \n",
    "    retval = []\n",
    "        \n",
    "    for record in records:\n",
    "        \n",
    "        if record.get('geohash'):\n",
    "            # get the geohash coordinate for the record\n",
    "            match = record.get('geohash')            \n",
    "            \n",
    "            # calculate the distance difference\n",
    "            x = (pygeohash.geohash_approximate_distance(match, search_hash[0:5]) / 1000)           \n",
    "            \n",
    "            # Get airports within 50 KM's\n",
    "            if match:\n",
    "                if x < 50:\n",
    "                    if record.get('geohash') not in retval:\n",
    "                        retval.append(record.get('geohash'))             \n",
    "                        return (record.get('src_airport', {}), retval)\n",
    "                        #print(retval)\n",
    "                else: \n",
    "                    pass\n",
    "            \n",
    "                \n",
    "        else:\n",
    "            pass            \n",
    "                            \n",
    "    \n",
    "airport_search(41.1499988, -95.91779)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conceptual-comparison",
   "metadata": {},
   "source": [
    "#### 3.2c Evolve the Avro Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stainless-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate Schema by writing the data to it to ensure it works\n",
    "\n",
    "def create_avro_dataset(records):\n",
    "    schema_path = schema_dir.joinpath('routesv2.avsc')\n",
    "    data_path = results_dir.joinpath('routesv2.avro')\n",
    "    \n",
    "    with open(schema_path, 'r') as f:\n",
    "        schema = json.load(f)\n",
    "    \n",
    "    parsed_schema = fastavro.parse_schema(schema)\n",
    "    \n",
    "    with open('routesv2.avro', 'wb') as out:\n",
    "        fastavro.writer(out, parsed_schema, records)\n",
    "        \n",
    "        \n",
    "create_avro_dataset(records) \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "heavy-nicaragua",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'airline': {'airline_id': 19016, 'name': 'Apache Air', 'alias': 'Apache', 'iata': 'ZM', 'icao': 'IWA', 'callsign': 'APACHE', 'country': 'United States', 'active': True}, 'src_airport': {'airport_id': 2913, 'name': 'Osh Airport', 'city': 'Osh', 'iata': 'OSS', 'icao': 'UAFO', 'latitude': 40.6090011597, 'longitude': 72.793296814, 'timezone': 6.0, 'dst': 'U', 'tz_id': 'Asia/Bishkek', 'type': 'airport', 'source': 'OurAirports', 'geohash': 'NONE'}, 'dst_airport': {'airport_id': 2912, 'name': 'Manas International Airport', 'city': 'Bishkek', 'iata': 'FRU', 'icao': 'UAFM', 'latitude': 43.0612983704, 'longitude': 74.4776000977, 'timezone': 6.0, 'dst': 'U', 'tz_id': 'Asia/Bishkek', 'type': 'airport', 'source': 'OurAirports', 'geohash': 'NONE'}, 'codeshare': False, 'stops': 0, 'equipment': ['734']}\n"
     ]
    }
   ],
   "source": [
    "# read first record to ensure geohash was in correct spots and data captured (probably better way but this was simple).\n",
    "\n",
    "data_path = 'results/routesv2.avro'\n",
    "with open(data_path, 'rb') as fo:\n",
    "    for record in reader(fo):\n",
    "        n = record                   # used n = record to stop after 1 record, if used print(record), would have gone on with 66,000 plus records\n",
    "\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legitimate-request",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
